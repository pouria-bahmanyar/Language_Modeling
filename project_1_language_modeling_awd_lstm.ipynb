{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrFCH9hpEyip"
   },
   "source": [
    "# 🔴 **Environment Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcNIGF4CUCk3"
   },
   "source": [
    "## 🟠 The command for connecting Colab to the local host (PC or laptop) is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9icRkJ8s8WE"
   },
   "source": [
    "`jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=4000 --NotebookApp.port_retries=0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFHw176FhqQa"
   },
   "source": [
    "## 🟠 Perhaps you may need to install the `torchmetrics` library. To do so, you can execute this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZsL_K3zhqew",
    "outputId": "48c636e5-99a9-473a-99a2-b6384d65ac9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/805.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/805.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m573.4/805.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWoVZ_MuKif0"
   },
   "source": [
    "## 🟠 Install `portalocker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjgdGoWmKYwV"
   },
   "outputs": [],
   "source": [
    "!pip install -q portalocker>=2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6tRkdc5HoZT"
   },
   "source": [
    "## 🟠 Install `wandb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbX6I2IJHoZV",
    "outputId": "2e484909-534f-4e1d-a512-64001c5452d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.15.11-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Collecting pathtools (from wandb)\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=a8881b2a591abacc88cc646d61dac7649ee98bf9ec27e8c2596b5b3d625dbaf3\n",
      "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.37 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.31.0 setproctitle-1.3.2 smmap-5.0.1 wandb-0.15.11\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_a3OXnSeV0z"
   },
   "source": [
    "# ⚠️ **Don't forget to restart the runtime!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNJRZe4QBudV"
   },
   "source": [
    "# 🔴 **Import Libs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhlVJEkJeTsV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import tqdm\n",
    "import torchmetrics as tm\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEzYlyeqTZqQ",
    "outputId": "07b5e4da-5a70-4809-cd88-24e32477c9dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DWjGTq6T8Jg",
    "outputId": "5b264ec7-7a06-440f-da3e-f444a4e957d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy --> 1.22.0\n",
      "torch --> 1.12.0\n",
      "torchtext --> 0.13.0\n",
      "tqdm --> 4.64.0\n"
     ]
    }
   ],
   "source": [
    "for lib in [np, torch, torchtext, tqdm]:\n",
    "  print(lib.__name__, '-->', lib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwaY_YcgRayy"
   },
   "source": [
    "# 🔴 **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yMS7bbmRayz"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpKbTUEIRayz"
   },
   "outputs": [],
   "source": [
    "def num_trainable_params(model):\n",
    "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
    "  return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6w6sLRLfw398"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "      torch.cuda.manual_seed(seed)\n",
    "      # torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "      # torch.backends.cudnn.deterministic = True\n",
    "      # torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwlVLNJXfUJw"
   },
   "source": [
    "# 🔴 **Arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqPVGv0TfUKE"
   },
   "outputs": [],
   "source": [
    "seed = 8\n",
    "\n",
    "batch_size = 80\n",
    "seq_len = 70\n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "num_layers = 3\n",
    "hidden_dim = 1150\n",
    "dropoute = 0.1\n",
    "dropouti = 0.65\n",
    "dropouth = 0.3\n",
    "dropouto = 0.4\n",
    "weight_drop = 0.\n",
    "\n",
    "lr = 30\n",
    "wd = 1.2e-6\n",
    "momentum = 0.9\n",
    "\n",
    "clip = 0.25\n",
    "\n",
    "wandb_enable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXOG_oXwrICX",
    "outputId": "1012874c-a35f-4429-f0d9-79b3717212e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input the WandB argument (run) name:1\n"
     ]
    }
   ],
   "source": [
    "wandb_arg_name = input('Please input the WandB argument (run) name:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vp5AUcSs-drV",
    "outputId": "604398cb-90e8-4584-f7bb-33144ba9df42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb_arg_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTql4Ftiunfr"
   },
   "source": [
    "# 🔴 **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujIVtjsYvxOI"
   },
   "source": [
    "## 🟠 Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ek9DpCNCChzF"
   },
   "source": [
    "🔰 In this session you should load WikiText2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShYpXvVzVmP6"
   },
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = WikiText2('/content/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCi-ofSLCzop"
   },
   "source": [
    "## 🟠 Build vocabulary and save it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L02PHFuyNRb3"
   },
   "source": [
    "🔰 In this section we need to:\n",
    "\n",
    "*   Define a tokenizer using `basic_english`\n",
    "*   Tokenize the dataset and collect tokens\n",
    "*   Build the vocabulary using `build_vocab_from_iterator`\n",
    "*   Manually insert special tokens and set the default index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mta7AoPki4wx"
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "torch.save(vocab, 'vocab.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rllml_H4jHvb",
    "outputId": "975aa29d-0bc9-42c5-d124-472af5ae7bfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 9206, 416]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['sajad', 'hi', 'how'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idRexFij4wgN"
   },
   "source": [
    "## 🟠 Transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VjvBOtvHu2v"
   },
   "source": [
    "🛑 Make sure to perform the transformations on train, validation and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApisIcGeGSsJ"
   },
   "source": [
    "🔰 Reshape the dataset into an `N x B x L` or `M x L` format, where `N` represents the number of batches, `B` is the batch size, `L` is the length of a sample within each batch, and `M` is equal to `N x B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocxM8YdsWH-1"
   },
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter, seq_len):\n",
    "  data = torch.cat([torch.LongTensor(vocab(tokenizer(line))) for line in raw_text_iter])\n",
    "\n",
    "  M = len(data) // seq_len\n",
    "\n",
    "  r = len(data) % seq_len\n",
    "  data = torch.cat((data, torch.LongTensor([0]))) if r==0 else data\n",
    "\n",
    "  inputs = data[:M*seq_len]\n",
    "  targets = data[1:M*seq_len+1]\n",
    "\n",
    "  inputs = inputs.reshape(-1, seq_len)\n",
    "  targets = targets.reshape(-1, seq_len)\n",
    "\n",
    "  return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1Am95DPRUOo",
    "outputId": "7958370b-b4cf-4797-c7cd-49641132fbe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([58569, 70]),\n",
       " torch.Size([58569, 70]),\n",
       " torch.Size([6125, 70]),\n",
       " torch.Size([6125, 70]),\n",
       " torch.Size([6909, 70]),\n",
       " torch.Size([6909, 70]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = data_process(train_iter, seq_len)\n",
    "X_valid, y_valid = data_process(valid_iter, seq_len)\n",
    "X_test, y_test = data_process(test_iter, seq_len)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgLgP04P4-aX"
   },
   "source": [
    "## 🟠 Custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkxH_IR2PBNq"
   },
   "source": [
    "🔰 Write a custom dataset class for LanguageModelDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cjpSkrtexap"
   },
   "outputs": [],
   "source": [
    "class LanguageModelDataset(Dataset):\n",
    "\n",
    "  def __init__(self, inputs, targets):\n",
    "    self.inputs = inputs\n",
    "    self.targets = targets\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.inputs.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0qUkL0CfQmr"
   },
   "outputs": [],
   "source": [
    "train_set = LanguageModelDataset(X_train, y_train)\n",
    "valid_set = LanguageModelDataset(X_valid, y_valid)\n",
    "test_set = LanguageModelDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92IiPxSMSPdz",
    "outputId": "3b1818bc-b1cb-461d-9ec8-571fbcd2f8c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    9,  3849,  3869,   881,     9, 20000,    83,  3849,    88,     0,\n",
       "          3869,    21,   780, 28780,     2,  6182,     3,  3849,     4,     1,\n",
       "          5023,    88,    20,     2,  1837,  1018,     7,    14,  3849,  3869,\n",
       "           881,   629,   976,     2,    23,     8,  5790,   299,    12,   575,\n",
       "           232,    67,   452,    19, 13722,     5,   757,     3,  2500,    17,\n",
       "             1,  1767,  5637,     3,   155,     6,   246,   354,     6,   976,\n",
       "             2,    24,    23,     1,   237,    67,     6,     1,  3849,    93]),\n",
       " tensor([ 3849,  3869,   881,     9, 20000,    83,  3849,    88,     0,  3869,\n",
       "            21,   780, 28780,     2,  6182,     3,  3849,     4,     1,  5023,\n",
       "            88,    20,     2,  1837,  1018,     7,    14,  3849,  3869,   881,\n",
       "           629,   976,     2,    23,     8,  5790,   299,    12,   575,   232,\n",
       "            67,   452,    19, 13722,     5,   757,     3,  2500,    17,     1,\n",
       "          1767,  5637,     3,   155,     6,   246,   354,     6,   976,     2,\n",
       "            24,    23,     1,   237,    67,     6,     1,  3849,    93,     3]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCQjacybOfqV"
   },
   "source": [
    "## 🟠 Define a dataloader if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqKMEyFNS-1a"
   },
   "source": [
    "🔰 Write dataloaders for the training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMCJ3UMD0U_f"
   },
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_lRY0FnUTDEN",
    "outputId": "bcfa8c22-2d89-4d33-f1d5-837dc9a6d099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([80, 70]),\n",
       " torch.Size([80, 70]),\n",
       " tensor([[  363,   299,     6,  ...,   319,    17,     8],\n",
       "         [  599,  5074,     6,  ...,   110,    14, 10855],\n",
       "         [ 2903,     3,    28,  ...,     2,    17,   617],\n",
       "         ...,\n",
       "         [  174,  3364,     3,  ...,   311,   871,     3],\n",
       "         [    4,   115,   952,  ...,     3,     1,   348],\n",
       "         [ 7155,     3,     9,  ...,     1,   749,     4]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch, y_batch = next(iter(train_loader))\n",
    "x_batch.shape, y_batch.shape, x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVoUEQm1yhNi",
    "outputId": "ebca389f-1061-4296-fde3-dd47f517be36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(363) tensor(299)\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "for inputs, targets in train_loader:\n",
    "  print(inputs[0, 0], targets[0, 0])\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ttl0AK3Hvyh"
   },
   "source": [
    "# 🔴 **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWDUTrsIzRhr"
   },
   "outputs": [],
   "source": [
    "class WeightDrop(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, module, weights, dropout=0):\n",
    "    super(WeightDrop, self).__init__()\n",
    "    self.module = module\n",
    "    self.weights = weights\n",
    "    self.dropout = dropout\n",
    "    self._setup()\n",
    "\n",
    "  def widget_demagnetizer_y2k_edition(*args, **kwargs):\n",
    "    return\n",
    "\n",
    "  def _setup(self):\n",
    "    if issubclass(type(self.module), torch.nn.RNNBase):\n",
    "      self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\n",
    "\n",
    "      for name_w in self.weights:\n",
    "        print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\n",
    "        w = getattr(self.module, name_w)\n",
    "        del self.module._parameters[name_w]\n",
    "        self.module.register_parameter(name_w + '_raw', nn.Parameter(w.data))\n",
    "\n",
    "  def _setweights(self):\n",
    "    for name_w in self.weights:\n",
    "      raw_w = getattr(self.module, name_w + '_raw')\n",
    "      w = None\n",
    "      # w = torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)\n",
    "      mask = torch.nn.functional.dropout(torch.ones_like(raw_w), p=self.dropout, training=True) * (1 - self.dropout)\n",
    "      setattr(self.module, name_w, raw_w * mask)\n",
    "\n",
    "  def forward(self, *args):\n",
    "    self._setweights()\n",
    "    return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2rOXE6mNe68"
   },
   "outputs": [],
   "source": [
    "def embedded_dropout(embed, words, dropout=0.1, scale=None):\n",
    "  if dropout:\n",
    "    mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(\n",
    "        embed.weight) / (1 - dropout)\n",
    "    masked_embed_weight = mask * embed.weight\n",
    "  else:\n",
    "    masked_embed_weight = embed.weight\n",
    "  if scale:\n",
    "    masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n",
    "\n",
    "  padding_idx = embed.padding_idx\n",
    "  if padding_idx is None:\n",
    "    padding_idx = -1\n",
    "\n",
    "  embedding = torch.nn.functional.embedding(words, masked_embed_weight,\n",
    "                                            padding_idx, embed.max_norm, embed.norm_type,\n",
    "                                            embed.scale_grad_by_freq, embed.sparse)\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwBQ3I-XLIHw"
   },
   "outputs": [],
   "source": [
    "class LockedDropout(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(LockedDropout, self).__init__()\n",
    "\n",
    "  def forward(self, x, dropout):\n",
    "    if not self.training or not dropout:\n",
    "      return x\n",
    "    m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
    "    mask = m.requires_grad_(False) / (1 - dropout)\n",
    "    mask = mask.expand_as(x)\n",
    "    return mask * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baOLnaB8jVC-"
   },
   "source": [
    "🔰 AWD-LSTM Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISnnHE0BMVqp"
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers,\n",
    "               dropoute=0.2, dropouti=0.2, dropouth=0.2, dropouto=0.2,\n",
    "               weight_drop=0.2):\n",
    "    super().__init__()\n",
    "    self.num_layers = num_layers\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embedding_dim = embedding_dim\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    self.lstms = []\n",
    "    self.lstms.append(nn.LSTM(embedding_dim, hidden_dim, num_layers=1, dropout=0, batch_first=False))\n",
    "    self.lstms.append(nn.LSTM(hidden_dim, hidden_dim, num_layers=1, dropout=0, batch_first=False))\n",
    "    self.lstms.append(nn.LSTM(hidden_dim, embedding_dim, num_layers=1, dropout=0, batch_first=False))\n",
    "    if weight_drop > 0:\n",
    "      self.lstms = [WeightDrop(lstm, ['weight_hh_l0'], dropout=weight_drop) for lstm in self.lstms]\n",
    "    self.lstms = nn.ModuleList(self.lstms)\n",
    "\n",
    "    self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    self.fc.weight = self.embedding.weight\n",
    "\n",
    "    self.lockdrop = LockedDropout()\n",
    "    self.dropoute = dropoute\n",
    "    self.dropouti = dropouti\n",
    "    self.dropouth = dropouth\n",
    "    self.dropouto = dropouto\n",
    "    # print(dropoute, dropouti, dropouth, dropouto)\n",
    "\n",
    "  def forward(self, src):\n",
    "    embedding = embedded_dropout(self.embedding, src, dropout=self.dropoute if self.training else 0)\n",
    "    embedding = self.lockdrop(embedding, self.dropouti)\n",
    "\n",
    "    new_hiddens = []\n",
    "    for l, lstm in enumerate(self.lstms):\n",
    "      embedding, _ = lstm(embedding)\n",
    "      if l != self.num_layers-1:\n",
    "        embedding = self.lockdrop(embedding, self.dropouth)\n",
    "\n",
    "    embedding = self.lockdrop(embedding, self.dropouto)\n",
    "\n",
    "    prediction = self.fc(embedding)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MgBVzorb9oQ",
    "outputId": "840a00a0-7c87-43c8-c073-340cc8909575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (embedding): Embedding(28782, 400)\n",
       "  (lstms): ModuleList(\n",
       "    (0): WeightDrop(\n",
       "      (module): LSTM(400, 1150)\n",
       "    )\n",
       "    (1): WeightDrop(\n",
       "      (module): LSTM(1150, 1150)\n",
       "    )\n",
       "    (2): WeightDrop(\n",
       "      (module): LSTM(1150, 400)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=400, out_features=28782, bias=True)\n",
       "  (lockdrop): LockedDropout()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "model = LanguageModel(vocab_size=len(vocab), embedding_dim=embedding_dim,\n",
    "                      hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "                      dropoute=dropoute, dropouti=dropouti,\n",
    "                      dropouth=dropouth, dropouto=dropouto,\n",
    "                      weight_drop=weight_drop, pretrained=pretrained)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "IOPLTufglrsy",
    "outputId": "d65eae88-c7d3-4870-cb19-93427cf7463e"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m, model\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mweight, model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39mweight\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\howsam-deep\\lib\\site-packages\\torch\\nn\\modules\\module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LanguageModel' object has no attribute 'weights'"
     ]
    }
   ],
   "source": [
    "model.weights, model.embedding.weight, model.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOpzY8sxbWYH",
    "outputId": "a37c84f2-0ace-445e-d61d-6e92913d396d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([70, 80, 28782]), torch.Size([80, 70]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_batch.t()).shape, x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xNyCwU8bKQ6"
   },
   "outputs": [],
   "source": [
    "num_trainable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GGav-WRoRYe"
   },
   "outputs": [],
   "source": [
    "num_trainable_params(model.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcNCBiTo0slj"
   },
   "outputs": [],
   "source": [
    "data_np = model.embedding.weight.cpu().detach().numpy()\n",
    "unique_rows, indices, counts = np.unique(data_np, axis=0, return_index=True, return_counts=True)\n",
    "len(unique_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4Mnh06_1sdO"
   },
   "outputs": [],
   "source": [
    "glove = GloVe(name='6B', dim=glove_dim)\n",
    "glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24qT-sgUO2-d"
   },
   "source": [
    "# 🔴 **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ma28M5Z36gsq",
    "outputId": "ab78cebb-295f-4384-f0c8-4c32aba79246"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ubk3xKaIG6i"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "metric = tm.text.Perplexity().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5znK6USrlVd5",
    "outputId": "9fa188bd-613c-4008-b89e-c280890aa001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key file does not exist. Please create the key file with your wandb API key.\n"
     ]
    }
   ],
   "source": [
    "key_file = '/content/key'\n",
    "\n",
    "if os.path.exists(key_file):\n",
    "    with open(key_file) as f:\n",
    "        key = f.readline().strip()\n",
    "    wandb.login(key=key)\n",
    "else:\n",
    "    print(\"Key file does not exist. Please create the key file with your wandb API key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0QNbC0YPCKZ"
   },
   "source": [
    "# 🔴 **Train ➰**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yS6EF4HUhi5e"
   },
   "source": [
    "🔰 This is the template for train function, change it if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WniOAgk0QyRI"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, loss_fn, optimizer, metric, epoch=None):\n",
    "  model.train()\n",
    "  loss_train = AverageMeter()\n",
    "  metric.reset()\n",
    "\n",
    "  with tqdm.tqdm(train_loader, unit='batch') as tepoch:\n",
    "    for inputs, targets in tepoch:\n",
    "      if epoch:\n",
    "        tepoch.set_description(f'Epoch {epoch}')\n",
    "\n",
    "      inputs = inputs.t().to(device)\n",
    "      targets = targets.t().to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      loss = loss_fn(outputs.reshape(-1, outputs.shape[-1]), targets.flatten())\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      nn.utils.clip_grad.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
    "\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss_train.update(loss.item(), n=len(targets))\n",
    "      metric.update(outputs, targets)\n",
    "\n",
    "      tepoch.set_postfix(loss=loss_train.avg, metric=metric.compute().item())\n",
    "\n",
    "  return model, loss_train.avg, metric.compute().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9HgVWslPGsH"
   },
   "source": [
    "# 🔴 **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsszJ7GVj2l3"
   },
   "source": [
    "🔰 This is the template for evaluation function, change it if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uV0_67_ZQ0xf"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, loss_fn, metric):\n",
    "  model.eval()\n",
    "  loss_eval = AverageMeter()\n",
    "  metric.reset()\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    for inputs, targets in test_loader:\n",
    "      inputs = inputs.t().to(device)\n",
    "      targets = targets.t().to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      loss = loss_fn(outputs.reshape(-1, outputs.shape[-1]), targets.flatten())\n",
    "      loss_eval.update(loss.item(), n=len(targets))\n",
    "\n",
    "      metric(outputs, targets)\n",
    "\n",
    "  return loss_eval.avg, metric.compute().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_5f69nwPtY2"
   },
   "source": [
    "# 🔴 **Training Process 〽️**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De7VreNxQdct"
   },
   "source": [
    "## 🟠 Finding Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpJ3wtyctQJH"
   },
   "source": [
    "### 🟡 **Step 1:** Calculate the loss for an untrained model using a few batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnE4F4GkzzaR"
   },
   "outputs": [],
   "source": [
    "model = LanguageModel(len(vocab), embedding_dim=300,\n",
    "                      hidden_dim=512, num_layers=2,\n",
    "                      dropout_embd=0.5, dropout_rnn=0.2).to(device)\n",
    "\n",
    "inputs, targets = next(iter(train_loader))\n",
    "inputs = inputs.to(device)\n",
    "targets = targets.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(inputs)\n",
    "  loss = loss_fn(outputs.reshape(-1, outputs.shape[-1]), targets.flatten())\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxHX7YRCBVxM"
   },
   "outputs": [],
   "source": [
    "outputs.reshape(-1, outputs.shape[-1]).shape, targets.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHj-Mp8FA_DW"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrHQCv7q7LF_"
   },
   "source": [
    "### 🟡 **Step 2:** Try to train and overfit the model on a small subset of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0ji0MXsWaPt"
   },
   "outputs": [],
   "source": [
    "model = LanguageModel(len(vocab), embedding_dim=embedding_dim,\n",
    "                      hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "                      dropout_embd=dropout_embd, dropout_rnn=dropout_rnn).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.9, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPRZQpPWJ2qv"
   },
   "outputs": [],
   "source": [
    "mini_train_size = 1000\n",
    "_, mini_train_dataset = random_split(train_set, (len(train_set)-mini_train_size, mini_train_size))\n",
    "mini_train_loader = DataLoader(mini_train_dataset, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FNIFCM6A6al"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "  model, _, _ = train_one_epoch(model, mini_train_loader, loss_fn, optimizer, metric, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLT4w0ZfAhlJ"
   },
   "source": [
    "### 🟡 **Step 3:** Train the model for a limited number of epochs, experimenting with various learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AqFqm1smGVrX",
    "outputId": "49004e92-4b8d-4a53-97fb-8b5731b5a868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR=20\n",
      ".\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                       | 0/367 [00:00<?, ?batch/s]C:\\Users\\PC\\anaconda3\\envs\\howsam-deep\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:968.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "100%|██████████████████████████████████████████████████| 367/367 [01:31<00:00,  4.02batch/s, loss=8.88, metric=7.16e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR=15\n",
      ".\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 367/367 [01:26<00:00,  4.24batch/s, loss=6.75, metric=857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR=10\n",
      ".\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 367/367 [01:20<00:00,  4.56batch/s, loss=6.75, metric=858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR=7.5\n",
      ".\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 367/367 [01:28<00:00,  4.16batch/s, loss=6.79, metric=894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR=5\n",
      ".\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 367/367 [01:28<00:00,  4.16batch/s, loss=6.84, metric=939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR=2.5\n",
      ".\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▉                                                   | 7/367 [00:01<01:40,  3.59batch/s, loss=9.56, metric=1.42e+4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwd, momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 16\u001b[0m   model, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, loss_fn, optimizer, metric, epoch)\u001b[0m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), targets\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39mclip)\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\howsam-deep\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\howsam-deep\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for lr in [20, 15, 10, 7.5, 5, 2.5]:\n",
    "  print(f'LR={lr}')\n",
    "\n",
    "  model = LanguageModel(vocab_size=len(vocab), embedding_dim=embedding_dim,\n",
    "                      hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "                      dropoute=dropoute, dropouti=dropouti,\n",
    "                      dropouth=dropouth, dropouto=dropouto,\n",
    "                      weight_drop=weight_drop, pretrained=True).to(device)\n",
    "  # model = torch.load('model.pt')\n",
    "\n",
    "  optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    model, _, _ = train_one_epoch(model, train_loader, loss_fn, optimizer, metric, epoch)\n",
    "\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC2GhaXfA8vC"
   },
   "source": [
    "### 🟡 Step 4: Create a small grid using the weight decay and the best learning rate.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a7UeNW3WWaPu",
    "outputId": "e51f3f4b-776d-4dc1-82d3-52aad3af3a7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR=7, WD=1.2e-06\n",
      ".\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 367/367 [01:28<00:00,  4.15batch/s, loss=6.68, metric=795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR=8, WD=1.2e-06\n",
      ".\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 367/367 [01:29<00:00,  4.11batch/s, loss=6.61, metric=745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR=14, WD=1.2e-06\n",
      ".\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▌                                                 | 11/367 [00:02<01:32,  3.83batch/s, loss=11.3, metric=8.28e+4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwd, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 16\u001b[0m   model, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, loss_fn, optimizer, metric, epoch)\u001b[0m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), targets\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39mclip)\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\howsam-deep\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\howsam-deep\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for lr in [7, 8, 14, 13, 12, 11, 10, 9]:\n",
    "  for wd in [1.2e-6]:\n",
    "    print(f'LR={lr}, WD={wd}')\n",
    "\n",
    "    model = LanguageModel(vocab_size=len(vocab), embedding_dim=embedding_dim,\n",
    "                      hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "                      dropoute=dropoute, dropouti=dropouti,\n",
    "                      dropouth=dropouth, dropouto=dropouto,\n",
    "                      weight_drop=weight_drop, pretrained=True).to(device)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      model, _, _ = train_one_epoch(model, train_loader, loss_fn, optimizer, metric, epoch)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mjd9Z3N1ef3I"
   },
   "source": [
    "### 🟡 Step 5: Train model for longer epochs using the best model from step 4.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWgkMgC6JWpU"
   },
   "outputs": [],
   "source": [
    "model = LanguageModel(len(vocab), embedding_dim=300,\n",
    "                      hidden_dim=512, num_layers=2,\n",
    "                      dropout_embd=0.5, dropout_rnn=0.2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "go-HitNQV_R1"
   },
   "outputs": [],
   "source": [
    "model = torch.load('/content/model-ppl_133.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVwLp-02JWpV"
   },
   "outputs": [],
   "source": [
    "lr = 3\n",
    "wd = 1e-6\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqxSVVB7JWpW"
   },
   "outputs": [],
   "source": [
    "loss_train_hist = []\n",
    "loss_valid_hist = []\n",
    "\n",
    "metric_train_hist = []\n",
    "metric_valid_hist = []\n",
    "\n",
    "best_loss_valid = torch.inf\n",
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVqS9SEPJWpW"
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "  # WandB\n",
    "  run = wandb.init(\n",
    "        project=\"language-modeling-lstms\",\n",
    "        config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"epochs\": num_epochs,\n",
    "        })\n",
    "\n",
    "  # Train\n",
    "  model, loss_train, metric_train = train_one_epoch(model,\n",
    "                                                    train_loader,\n",
    "                                                    loss_fn,\n",
    "                                                    optimizer,\n",
    "                                                    metric,\n",
    "                                                    epoch)\n",
    "  # Validation\n",
    "  loss_valid, metric_valid = evaluate(model,\n",
    "                                      valid_loader,\n",
    "                                      loss_fn,\n",
    "                                      metric)\n",
    "\n",
    "  loss_train_hist.append(loss_train)\n",
    "  loss_valid_hist.append(loss_valid)\n",
    "\n",
    "  metric_train_hist.append(metric_train)\n",
    "  metric_valid_hist.append(metric_valid)\n",
    "\n",
    "  if loss_valid < best_loss_valid:\n",
    "    torch.save(model, f'model.pt')\n",
    "    best_loss_valid = loss_valid\n",
    "    print('Model Saved!')\n",
    "\n",
    "  print(f'Valid: Loss = {loss_valid:.4}, Metric = {metric_valid:.4}')\n",
    "  print()\n",
    "\n",
    "  epoch_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjGQ-M02cusP"
   },
   "source": [
    "## 🟠 Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsWyc30h3mef"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGRWkn5wm9oP"
   },
   "source": [
    "🔰 Define train dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uF3H9DeD6TCn"
   },
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8cmqYrL6UzZ"
   },
   "source": [
    "🔰 Define model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCtZXDybxexf",
    "outputId": "3f1837b2-2028-44f9-e381-fd9e9dffd582"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (embedding): Embedding(28782, 400)\n",
       "  (lstms): ModuleList(\n",
       "    (0): LSTM(400, 1150)\n",
       "    (1): LSTM(1150, 1150)\n",
       "    (2): LSTM(1150, 400)\n",
       "  )\n",
       "  (fc): Linear(in_features=400, out_features=28782, bias=True)\n",
       "  (lockdrop): LockedDropout()\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "model = LanguageModel(vocab_size=len(vocab), embedding_dim=embedding_dim,\n",
    "                      hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "                      dropoute=dropoute, dropouti=dropouti,\n",
    "                      dropouth=dropouth, dropouto=dropouto,\n",
    "                      weight_drop=weight_drop, pretrained=pretrained).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Klf6g_N8c-Ub"
   },
   "outputs": [],
   "source": [
    "# model = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUKZRiQPxqrB"
   },
   "source": [
    "🔰 Define optimizer and Set learning rate and weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bowjVB5yIXUP",
    "outputId": "2d5de7a9-274f-4ba9-8856-36fb255c5405"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    foreach: None\n",
       "    lr: 7.5\n",
       "    maximize: False\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 1.2e-06\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "lr = 7.5\n",
    "# wd = 1e-6\n",
    "# momentum = 0.9\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
    "# optimizer = optim.SGD([{'params': model.embedding.parameters(), 'lr': 0.1*lr},\n",
    "#                        {'params': model.lstms.parameters(), 'lr': lr}],\n",
    "#                       weight_decay=wd, momentum=momentum)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AdYaMU4x34g"
   },
   "source": [
    "🔰 Initialize `wandb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231,
     "referenced_widgets": [
      "fb6dedef28f445c788ef96c6ef94e394",
      "a5e502fbb6ff4009a40ebb7f78350389",
      "e25e962434524b8ca29332270f825374",
      "f842f9b79f934455aa6046079c465db5",
      "063371886514435e8faf69ef9b840721",
      "d9f192c8de064e729138482fa02f6990",
      "275c47b7091440e8b9bbed17b01ed67c",
      "211c82f9961745158e92db1367734f3f"
     ]
    },
    "id": "0yboUzafnGD8",
    "outputId": "bc7dd3d8-508d-4e45-a9ad-92e2ea1772f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    body {\n",
       "      font-size: 24px;\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6dedef28f445c788ef96c6ef94e394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112884877777812, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20230927_225650-8kru5fap</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/howsam/LM-AWD-LSTM/runs/8kru5fap' target=\"_blank\">weight-drop</a></strong> to <a href='https://wandb.ai/howsam/LM-AWD-LSTM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/howsam/LM-AWD-LSTM' target=\"_blank\">https://wandb.ai/howsam/LM-AWD-LSTM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/howsam/LM-AWD-LSTM/runs/8kru5fap' target=\"_blank\">https://wandb.ai/howsam/LM-AWD-LSTM/runs/8kru5fap</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if wandb_enable:\n",
    "  wandb.init(\n",
    "      project='LM-AWD-LSTM',\n",
    "      name=wandb_arg_name,\n",
    "      config={\n",
    "          'lr': lr,\n",
    "          'momentum': momentum,\n",
    "          'batch_size': batch_size,\n",
    "          'seq_len': seq_len,\n",
    "          'hidden_dim': hidden_dim,\n",
    "          'embedding_dim': embedding_dim,\n",
    "          'num_layers': num_layers,\n",
    "          'dropout_embed': dropoute,\n",
    "          'dropout_in_lstm': dropouti,\n",
    "          'dropout_h_lstm': dropouth,\n",
    "          'dropout_out_lstm': dropouto,\n",
    "          'clip': clip,\n",
    "      }\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUyFFIzlyaiB"
   },
   "source": [
    "🔰 Write code to train the model for `num_epochs` epoches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAXagB4yvtZd"
   },
   "outputs": [],
   "source": [
    "loss_train_hist = []\n",
    "loss_valid_hist = []\n",
    "\n",
    "metric_train_hist = []\n",
    "metric_valid_hist = []\n",
    "\n",
    "best_loss_valid = torch.inf\n",
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PovABWnU3ld0",
    "outputId": "207b76c5-1d12-4f63-f5d1-77bda60217b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████████| 733/733 [02:29<00:00,  4.91batch/s, loss=6.31, metric=552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 5.503, Metric = 245.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████████████████████████| 733/733 [02:29<00:00,  4.89batch/s, loss=5.61, metric=275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 5.132, Metric = 169.7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.88batch/s, loss=5.33, metric=207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.936, Metric = 139.4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.87batch/s, loss=5.16, metric=173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.838, Metric = 126.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.88batch/s, loss=5.04, metric=155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.805, Metric = 122.3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.88batch/s, loss=4.95, metric=141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.726, Metric = 113.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.87batch/s, loss=4.88, metric=132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.693, Metric = 109.3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|█████████████████████████████████████████████| 733/733 [02:31<00:00,  4.83batch/s, loss=4.82, metric=124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.678, Metric = 107.6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.86batch/s, loss=4.77, metric=118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.662, Metric = 105.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|████████████████████████████████████████████| 733/733 [02:30<00:00,  4.87batch/s, loss=4.73, metric=113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.637, Metric = 103.3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.86batch/s, loss=4.7, metric=109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Loss = 4.651, Metric = 104.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|████████████████████████████████████████████| 733/733 [02:30<00:00,  4.86batch/s, loss=4.67, metric=106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.617, Metric = 101.3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|████████████████████████████████████████████| 733/733 [02:30<00:00,  4.86batch/s, loss=4.64, metric=104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.605, Metric = 100.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|████████████████████████████████████████████| 733/733 [02:30<00:00,  4.87batch/s, loss=4.61, metric=101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.6, Metric = 99.56\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|███████████████████████████████████████████| 733/733 [02:30<00:00,  4.88batch/s, loss=4.59, metric=98.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.578, Metric = 97.46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|███████████████████████████████████████████| 733/733 [02:30<00:00,  4.87batch/s, loss=4.57, metric=96.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Loss = 4.59, Metric = 98.56\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|███████████████████████████████████████████| 733/733 [02:30<00:00,  4.87batch/s, loss=4.56, metric=95.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.574, Metric = 97.03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|███████████████████████████████████████████| 733/733 [02:29<00:00,  4.90batch/s, loss=4.54, metric=93.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Valid: Loss = 4.566, Metric = 96.32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|███████████████████████████████████████████| 733/733 [02:29<00:00,  4.91batch/s, loss=4.52, metric=92.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Loss = 4.573, Metric = 97.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|█████████████████████████████████████████████| 733/733 [02:29<00:00,  4.91batch/s, loss=4.51, metric=91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Loss = 4.568, Metric = 96.51\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|████████████████████████████████████████████| 733/733 [02:30<00:00,  4.86batch/s, loss=4.5, metric=90.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Loss = 4.568, Metric = 96.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22:  10%|████▌                                       | 75/733 [00:15<02:19,  4.73batch/s, loss=4.46, metric=86.9]"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "  # Train\n",
    "  model, loss_train, metric_train = train_one_epoch(model,\n",
    "                                                    train_loader,\n",
    "                                                    loss_fn,\n",
    "                                                    optimizer,\n",
    "                                                    metric,\n",
    "                                                    epoch)\n",
    "  # Validation\n",
    "  loss_valid, metric_valid = evaluate(model,\n",
    "                                      valid_loader,\n",
    "                                      loss_fn,\n",
    "                                      metric)\n",
    "\n",
    "  loss_train_hist.append(loss_train)\n",
    "  loss_valid_hist.append(loss_valid)\n",
    "\n",
    "  metric_train_hist.append(metric_train)\n",
    "  metric_valid_hist.append(metric_valid)\n",
    "\n",
    "  if loss_valid < best_loss_valid:\n",
    "    torch.save(model, f'model.pt')\n",
    "    best_loss_valid = loss_valid\n",
    "    print('Model Saved!')\n",
    "\n",
    "  print(f'Valid: Loss = {loss_valid:.4}, Metric = {metric_valid:.4}')\n",
    "  print()\n",
    "\n",
    "  if wandb_enable:\n",
    "    wandb.log({\"metric_train\": metric_train, \"loss_train\": loss_train,\n",
    "                \"metric_valid\": metric_valid, \"loss_valid\": loss_valid})\n",
    "\n",
    "  epoch_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "JDKmOvCY2AYX",
    "outputId": "1177c90d-5626-414f-afd2-d76728e3727c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    body {\n",
       "      font-size: 24px;\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">weight-drop</strong> at: <a href='https://wandb.ai/howsam/LM-AWD-LSTM/runs/g9ybeitf' target=\"_blank\">https://wandb.ai/howsam/LM-AWD-LSTM/runs/g9ybeitf</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230927_225610-g9ybeitf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oK20iNRI3Xxb"
   },
   "source": [
    "## 🟠 Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKlLvCwuzEAA"
   },
   "source": [
    "🔰 Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYFzTsdIOkVp"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(range(epoch_counter), loss_train_hist, 'r-', label='Train')\n",
    "plt.plot(range(epoch_counter), loss_valid_hist, 'b-', label='Validation')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZ9UIdmkfxlA"
   },
   "source": [
    "# 🔴 **Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SO8iPWH1zVYn"
   },
   "source": [
    "🔰 Test your model using data from the test set and images that are not present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09Q1Cwaa6sGb"
   },
   "outputs": [],
   "source": [
    "model_path = 'model.pt'\n",
    "model = torch.load(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dgtH46XBWPF"
   },
   "outputs": [],
   "source": [
    "loss_valid, metric_valid = evaluate(model, valid_loader, loss_fn, metric)\n",
    "metric_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35sn67IhKcm_"
   },
   "outputs": [],
   "source": [
    "loss_test, metric_test = evaluate(model, test_loader, loss_fn, metric)\n",
    "metric_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzcQQwFuar_7"
   },
   "source": [
    "# 🔴 **Generate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jh2_9jUp0GF4"
   },
   "source": [
    "🔰 Your mission is to write a `generate` function and use a desired sentence to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pskvb--R-wJ0"
   },
   "outputs": [],
   "source": [
    "model_path = 'model.pt'\n",
    "model = torch.load(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsTQu0p6RsgO"
   },
   "outputs": [],
   "source": [
    "prompt = 'In a galaxy far, far away, there'\n",
    "\n",
    "indices = vocab(tokenizer(prompt))\n",
    "itos = vocab.get_itos()\n",
    "\n",
    "max_seq_len = 35\n",
    "for i in range(max_seq_len):\n",
    "  src = torch.LongTensor(indices).to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    prediction = model(src)\n",
    "\n",
    "  # Method 1\n",
    "  # idx = torch.argmax(prediction[-1])\n",
    "  # itos = vocab.get_itos()\n",
    "  # itos[idx]\n",
    "\n",
    "  # Method 2\n",
    "  temperature = 0.5\n",
    "  probs = torch.softmax(prediction[-1]/temperature, dim=0)\n",
    "\n",
    "  idx = vocab['<ukn>']\n",
    "  while idx == vocab['<ukn>']:\n",
    "    idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "  token = itos[idx]\n",
    "  prompt += ' ' + token\n",
    "\n",
    "  if idx == vocab['.']:\n",
    "    break\n",
    "\n",
    "  indices.append(idx)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5SvSDLal8YB"
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, seed=None):\n",
    "  if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "  indices = vocab(tokenizer(prompt))\n",
    "  itos = vocab.get_itos()\n",
    "\n",
    "  for i in range(max_seq_len):\n",
    "    src = torch.LongTensor(indices).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      prediction = model(src)\n",
    "\n",
    "    # Method 1\n",
    "    # idx = torch.argmax(prediction[-1])\n",
    "    # itos = vocab.get_itos()\n",
    "    # itos[idx]\n",
    "\n",
    "    # Method 2\n",
    "    probs = torch.softmax(prediction[-1]/temperature, dim=0)\n",
    "\n",
    "    idx = vocab['<ukn>']\n",
    "    while idx == vocab['<ukn>']:\n",
    "      idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    token = itos[idx]\n",
    "    prompt += ' ' + token\n",
    "\n",
    "    if idx == vocab['.']:\n",
    "      return prompt\n",
    "\n",
    "    indices.append(idx)\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_Cw2bzfRmY9"
   },
   "outputs": [],
   "source": [
    "prompt = 'In a galaxy far, far away, there'\n",
    "prompt = 'The sun was setting in the'\n",
    "prompt = 'Once upon a time, there lived a young princess named'\n",
    "prompt = 'What is the meaning '\n",
    "\n",
    "generate(prompt, 35, 0.5, model, tokenizer, vocab)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VrFCH9hpEyip",
    "UcNIGF4CUCk3",
    "AFHw176FhqQa",
    "uWoVZ_MuKif0",
    "t6tRkdc5HoZT",
    "BNJRZe4QBudV",
    "RwaY_YcgRayy",
    "3in1e9BksgIh",
    "RTql4Ftiunfr",
    "ujIVtjsYvxOI",
    "wCi-ofSLCzop",
    "idRexFij4wgN",
    "PgLgP04P4-aX",
    "NCQjacybOfqV",
    "3ttl0AK3Hvyh",
    "24qT-sgUO2-d",
    "W0QNbC0YPCKZ",
    "G9HgVWslPGsH",
    "lpJ3wtyctQJH",
    "BrHQCv7q7LF_",
    "BLT4w0ZfAhlJ",
    "uC2GhaXfA8vC",
    "Mjd9Z3N1ef3I",
    "rjGQ-M02cusP",
    "oK20iNRI3Xxb",
    "KZ9UIdmkfxlA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
